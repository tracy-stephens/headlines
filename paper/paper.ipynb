{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by Topic and Author Detection of News Articles with DBCSAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The underlying data is New York Times articles between January 2000 and June 2020 obtained through their free and public [Archive API](https://developer.nytimes.com/docs/archive-product/1/overview). To limit the scope of the analysis, I limited the articles to those with a particular keyword. For the baseline version of the model, these are only articles that contain the keyword 'Brazil'.\n",
    "\n",
    "Headlines vs. Abstracts\n",
    "The average headline is 8.7 words long, and the average abstract is 35.8 tokens words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model: GloVe\n",
    "\n",
    "For the baseline version of the model, I use pre-trained word vectors from the Global Vectors for Word Representation algorithm. These vectors were trained on data from \n",
    "\n",
    "Context-free - single word embedding representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Embeddings\n",
    "\n",
    "In the baseline version, we take the average embedding across each headline. The has the problem that more frequent words are given as much significance as less frequent words. \n",
    "\n",
    "https://openreview.net/forum?id=SyK00v5xx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Cosine similarity vs. Euclidian distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Evaluating performance of unsupervied algorithms is not as straightforward as it is for supervised algorithms. Previously used metrics for evaluting clustering algorithms generally fall into one of two categories: those that require an external ground-truth measure of similarity, such as class labels, and those that do not. \n",
    "\n",
    "Cluster Validity Indices\n",
    "\n",
    "This [2011 paper](https://arxiv.org/pdf/1507.03340.pdf) summarizes how these various metrics compare across different five different clustering algorithms applied to user data. The relative performance of each clustering algorithm differed substantially depending on the metric applied. In their research DBSCAN tended to appear to perform relatively better on x, x, and x, and worse on x, x, and x. This could be tied to the fact that DBSCAN clusters may be non-regular shapes and sizes and may not have a clear center.\n",
    "\n",
    "In the first category, \n",
    "\n",
    "### Performance Metrics with no External Measure of Similarity\n",
    "\n",
    "https://arxiv.org/pdf/1507.03340.pdf\n",
    "These techniques compare inter\n",
    "\n",
    "\n",
    "1. **Dunn's Index** is higher when clusters are more dense and further apart. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Cluster Perfomance Metrics\n",
    "\n",
    "- Compare the cluster labels with the author\n",
    "- Compare the cluster labels with the other keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
    "\n",
    "Transformers include contextual relationships\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
