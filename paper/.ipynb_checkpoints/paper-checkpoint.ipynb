{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by Topic Detection of New York Times with DBCSAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The underlying data is New York Times articles between January 2000 and June 2020 obtained through their free and public [Archive API](https://developer.nytimes.com/docs/archive-product/1/overview). To limit the scope of the analysis, I limited the articles to those with a particular keyword. For the baseline version of the model, these are only articles that contain the keyword 'Brazil'.\n",
    "\n",
    "Headlines vs. Abstracts\n",
    "The average headline is 8.7 words long, and the average abstract is 35.8 tokens words.\n",
    "\n",
    "* Show different types of keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model: GloVe\n",
    "\n",
    "For the baseline version of the model, I use pre-trained word vectors from the Global Vectors for Word Representation algorithm. These vectors were trained on data from \n",
    "\n",
    "Context-free - single word embedding representation\n",
    "https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline Embeddings\n",
    "\n",
    "In the baseline version, we take the average embedding across each headline. The has the problem that more frequent words are given as much significance as less frequent words. \n",
    "\n",
    "https://openreview.net/forum?id=SyK00v5xx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Cosine similarity vs. Euclidian distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Evaluating performance of unsupervied algorithms is not as straightforward as it is for supervised algorithms. Previously used metrics for evaluting clustering algorithms generally fall into one of two categories: those that require an external ground-truth measure of similarity, such as class labels, and those that do not. \n",
    "\n",
    "Cluster Validity Indices\n",
    "\n",
    "This [2011 paper](https://arxiv.org/pdf/1507.03340.pdf) summarizes how these various metrics compare across different five different clustering algorithms applied to user data. The relative performance of each clustering algorithm differed substantially depending on the metric applied. In their research DBSCAN tended to appear to perform relatively better on x, x, and x, and worse on x, x, and x. This could be tied to the fact that DBSCAN clusters may be non-regular shapes and sizes and may not have a clear center.\n",
    "\n",
    "In the first category, sever\n",
    "\n",
    "### Internal Cluster Validation Metrics\n",
    "\n",
    "https://arxiv.org/pdf/1507.03340.pdf\n",
    "These techniques compare inter\n",
    "\n",
    "1. **Silhouette Index** evaluates cluster validity based on the average distances of each point. For each datapoint $i$, we compute: $$S_i = \\frac{b_i-a_i}{max(a_i, b_i)}$$ where $a_i$ is the average distance of the point to all other points in its cluster, and $b_i$ is the average distance of the point to each point outside the cluster. The final index is computed: $$C = \\frac{S-S_{min}}{S_{max} - S_{min}}$$ For any given point, $b_i-a_i$ can be negative since the DBSCAN algorithm can accommadate clusters of irregular sizes. In this case, model iterations with very low levels of epsilon resulted in negative Silhouette Indices. \n",
    "\n",
    "http://datamining.rutgers.edu/publication/internalmeasures.pdf\n",
    "2. The **Calinski Harabasz Index** compares the average sum of squares within each cluster to the average sum of squares between each cluster: $$CH=\\frac{\\frac{SSB}{k-1}}{\\frac{SSW}{N-k}}$$ where SSB is the sum of within the clusters, and SSB is the sum of squares between the clusters.\n",
    "\n",
    "Since DBSCAN does not produce convex clusters, it makes more sense to use a density-based cluster validity measure. In other words, because DBSCAN aims to achieve the highest level of intra-cluster density, rather than the highest level of closeness to a central point, cluster validity indices that account for density are more appropriate than those that only account for the intra-cluster distance between all points in a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pdfs.semanticscholar.org/581c/71da74bd3baa06693cc6d0751e7c60f81bb3.pdf\n",
    "3. **SD** compare the average \"scattering\" to the total separation of the clusters: $SD = \\alpha Scatt + Dist$ where $Scatt = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\lVert\\sigma(v_i)\\rVert}{\\lVert\\sigma(x)\\rVert}}$ with $\\sigma(v_i)$ being the variance of cluster $i$ and $\\sigma(x)$ being the variance of the dataset.\n",
    "\n",
    "\n",
    "https://www.researchgate.net/publication/3940215_Clustering_Validity_Assessment_Finding_the_optimal_partitioning_of_a_data_set\n",
    "4. **S Dbw** compares cluster compactness, measured by intra-cluster variance, vs cluster separation, measured by inter-cluster density. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Cluster Validation Metrics\n",
    "\n",
    "- Compare the cluster labels with the author\n",
    "- Compare the cluster labels with the other keywords\n",
    "\n",
    "### Relative Cluster Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
    "\n",
    "Transformers include contextual relationships\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
